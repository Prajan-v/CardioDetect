{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CardioDetect – MLP Experiments\n",
    "\n",
    "Exploratory notebook to test MLP architectures, regularization, and threshold tuning on the official train/val/test splits used by the tuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n",
    "import sys\n",
    "\n",
    "# Make src/ importable (so we can reuse mlp_tuning utilities)\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.mlp_tuning import load_splits, encode_categorical_features\n",
    "\n",
    "print('Imports OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load official train/val/test splits and encode any categorical features\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_splits()\n",
    "X_train, X_val, X_test = encode_categorical_features(X_train, X_val, X_test)\n",
    "\n",
    "print(f'Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Scale features for MLP\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print('Features scaled for MLP')\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Helper to compute and print metrics on a split\n",
    "def eval_split(model, X, y, name):\n",
    "    y_pred = model.predict(X)\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    rec = recall_score(y, y_pred)\n",
    "    prec = precision_score(y, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y, y_proba)\n",
    "    print(f"{name}: Acc={acc:.4f}, Rec={rec:.4f}, Prec={prec:.4f}, F1={f1:.4f}, AUC={auc:.4f}")\n",
    "    return acc, rec, prec, f1, auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Baseline MLP (128-64-32, alpha=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Baseline notebook-style MLP\n",
    "baseline_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(128, 64, 32),\n",
    "    alpha=1e-4,\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=500,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "baseline_mlp.fit(X_train_s, y_train)\n",
    "\n",
    "print('Baseline MLP metrics:')\n",
    "baseline_train = eval_split(baseline_mlp, X_train_s, y_train, 'Train')\n",
    "baseline_val = eval_split(baseline_mlp, X_val_s, y_val, 'Val')\n",
    "baseline_test = eval_split(baseline_mlp, X_test_s, y_test, 'Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Small MLP grid (architecture & L2)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compare a few nearby architectures / regularization settings\n",
    "candidates = [\n",
    "    {\"name\": \"baseline_128-64-32_a1e-4\", \"hidden_layer_sizes\": (128, 64, 32), \"alpha\": 1e-4},\n",
    "    {\"name\": \"256-128-64_a1e-4\",         \"hidden_layer_sizes\": (256, 128, 64), \"alpha\": 1e-4},\n",
    "    {\"name\": \"256-128-64_a1e-3\",         \"hidden_layer_sizes\": (256, 128, 64), \"alpha\": 1e-3},\n",
    "    {\"name\": \"128-64_a1e-3\",             \"hidden_layer_sizes\": (128, 64),      \"alpha\": 1e-3},\n",
    "]\n",
    "\n",
    "rows = []\n",
    "baseline_recall = 0.919  # tuned MLP test recall (guardrail)\n",
    "\n",
    "for cfg in candidates:\n",
    "    print(f\\nTraining candidate: {cfg['name']}...)\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=cfg['hidden_layer_sizes'],\n",
    "        alpha=cfg['alpha'],\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "    mlp.fit(X_train_s, y_train)\n",
    "\n",
    "    metrics = {}\n",
    "    for split_name, X_split, y_split in [\n",
    "        ('Train', X_train_s, y_train),\n",
    "        ('Val',   X_val_s,   y_val),\n",
    "        ('Test',  X_test_s,  y_test),\n",
    "    ]:\n",
    "        y_pred = mlp.predict(X_split)\n",
    "        acc = accuracy_score(y_split, y_pred)\n",
    "        rec = recall_score(y_split, y_pred)\n",
    "        metrics[f'{split_name}_acc'] = acc\n",
    "        metrics[f'{split_name}_rec'] = rec\n",
    "\n",
    "    row = {\n",
    "        'Name': cfg['name'],\n",
    "        'hidden_layer_sizes': cfg['hidden_layer_sizes'],\n",
    "        'alpha': cfg['alpha'],\n",
    "        **metrics,\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "grid_df = pd.DataFrame(rows)\n",
    "print('\\nMLP small grid – Train/Val/Test metrics')\n",
    "print(grid_df.round(4).to_string(index=False))\n",
    "\n",
    "valid = grid_df[(grid_df['Val_rec'] >= baseline_recall) & (grid_df['Test_rec'] >= baseline_recall)]\n",
    "if not valid.empty:\n",
    "    print('\\nCandidates meeting Val & Test recall >= 0.919:')\n",
    "    print(valid[['Name', 'Val_acc', 'Val_rec', 'Test_acc', 'Test_rec']].round(4).to_string(index=False))\n",
    "else:\n",
    "    print('\\nNo candidate reached Val & Test recall >= 0.919 (tuned baseline).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Threshold tuning for baseline MLP (recall guardrail)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Use validation set to choose a decision threshold that keeps recall >= 0.919\n",
    "# and maximizes validation accuracy, then report test metrics at that threshold.\n",
    "\n",
    "baseline_recall = 0.919\n",
    "y_proba_val = baseline_mlp.predict_proba(X_val_s)[:, 1]\n",
    "\n",
    "best_acc = -1.0\n",
    "best_thresh = 0.5\n",
    "\n",
    "for thresh in np.linspace(0.1, 0.9, 81):\n",
    "    y_pred_val = (y_proba_val >= thresh).astype(int)\n",
    "    rec = recall_score(y_val, y_pred_val)\n",
    "    acc = accuracy_score(y_val, y_pred_val)\n",
    "    if rec >= baseline_recall and acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_thresh = thresh\n",
    "\n",
    "if best_acc < 0:\n",
    "    print('No validation threshold achieved recall >= baseline (0.919). Keeping default 0.5.')\n",
    "    best_thresh = 0.5\n",
    "else:\n",
    "    print(f'Chosen threshold={best_thresh:.3f} with Val Acc={best_acc:.4f}, Val Recall>= {baseline_recall}')\n",
    "\n",
    "y_proba_test = baseline_mlp.predict_proba(X_test_s)[:, 1]\n",
    "y_pred_test = (y_proba_test >= best_thresh).astype(int)\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_rec = recall_score(y_test, y_pred_test)\n",
    "print(f'Test Accuracy: {test_acc:.4f}, Test Recall: {test_rec:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
