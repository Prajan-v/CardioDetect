{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CardioDetect – Systematic MLP Grid Search & Threshold Tuning\n",
    "\n",
    "**Objective**: Explore MLP architectures to beat `mlp_v2` (Acc=0.9359, Recall=0.9190) while maintaining recall ≥ 0.9190.\n",
    "\n",
    "**Grid**: 5 architectures × 4 alphas × 2 learning rates × 2 max_iters = 80 experiments\n",
    "\n",
    "**Output (this notebook)**: structured CSV with metrics for all experiments.\n",
    "\n",
    "**Note**: Obsidian notes and leaderboards are generated by a separate script (no Obsidian writes from this notebook).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n",
      "Obsidian output: /Users/prajanv/CardioDetect/obsidian_notes/experiments_mlp\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, recall_score, precision_score, \n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import sys\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "from src.mlp_tuning import load_splits, encode_categorical_features\n",
    "\n",
    "# Where to store raw experiment results (CSV only, no Obsidian coupling)\n",
    "RESULTS_DIR = Path.cwd().parent / 'output' / 'mlp_experiments'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Imports OK')\n",
    "print(f'Results will be saved to: {RESULTS_DIR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 11286 | Val: 2418 | Test: 2419\n",
      "Features: 179\n"
     ]
    }
   ],
   "source": [
    "# Load data and scale features\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_splits()\n",
    "X_train, X_val, X_test = encode_categorical_features(X_train, X_val, X_test)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_val_s = scaler.transform(X_val)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "\n",
    "print(f'Train: {len(X_train)} | Val: {len(X_val)} | Test: {len(X_test)}')\n",
    "print(f'Features: {X_train_s.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total experiments to run: 80\n",
      "Baseline: Acc=0.9359, Recall=0.919\n",
      "Constraint: Recall >= 0.919\n"
     ]
    }
   ],
   "source": [
    "# ============ CONFIGURATION ============\n",
    "BASELINE_TEST_ACC = 0.9359\n",
    "BASELINE_TEST_RECALL = 0.9190\n",
    "RECALL_CONSTRAINT = 0.9190  # Hard constraint\n",
    "\n",
    "# Hyperparameter search space\n",
    "HIDDEN_LAYER_SIZES = [\n",
    "    (128, 64, 32),\n",
    "    (256, 128, 64),\n",
    "    (256, 256, 128),\n",
    "    (128, 64),\n",
    "    (64, 32),\n",
    "]\n",
    "ALPHAS = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "LEARNING_RATES = [0.001, 0.0007]\n",
    "MAX_ITERS = [300, 500]\n",
    "\n",
    "# Calculate total experiments\n",
    "total_experiments = len(HIDDEN_LAYER_SIZES) * len(ALPHAS) * len(LEARNING_RATES) * len(MAX_ITERS)\n",
    "print(f'Total experiments to run: {total_experiments}')\n",
    "print(f'Baseline: Acc={BASELINE_TEST_ACC}, Recall={BASELINE_TEST_RECALL}')\n",
    "print(f'Constraint: Recall >= {RECALL_CONSTRAINT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# ============ HELPER FUNCTIONS ============\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    \"\"\"Compute all metrics for a split.\"\"\"\n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'recall': recall_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'auc': roc_auc_score(y_true, y_proba),\n",
    "    }\n",
    "\n",
    "def eval_model_at_threshold(model, X, y, threshold=0.5):\n",
    "    \"\"\"Evaluate model at a specific threshold.\"\"\"\n",
    "    y_proba = model.predict_proba(X)[:, 1]\n",
    "    y_pred = (y_proba >= threshold).astype(int)\n",
    "    return compute_metrics(y, y_pred, y_proba)\n",
    "\n",
    "def find_best_threshold(model, X_val, y_val, recall_floor=0.9190):\n",
    "    \"\"\"\n",
    "    Find threshold that maximizes validation accuracy while maintaining recall >= recall_floor.\n",
    "    Returns (best_threshold, best_metrics_at_threshold).\n",
    "    \"\"\"\n",
    "    y_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    best_thresh = 0.5\n",
    "    best_acc = -1.0\n",
    "    best_rec = 0.0\n",
    "    \n",
    "    for thresh in np.linspace(0.1, 0.9, 81):\n",
    "        y_pred = (y_proba >= thresh).astype(int)\n",
    "        rec = recall_score(y_val, y_pred)\n",
    "        acc = accuracy_score(y_val, y_pred)\n",
    "        \n",
    "        if rec >= recall_floor:\n",
    "            if acc > best_acc or (acc == best_acc and rec > best_rec):\n",
    "                best_acc = acc\n",
    "                best_rec = rec\n",
    "                best_thresh = thresh\n",
    "    \n",
    "    # If no threshold meets constraint, return default 0.5\n",
    "    if best_acc < 0:\n",
    "        best_thresh = 0.5\n",
    "    \n",
    "    # Compute full metrics at best threshold\n",
    "    y_pred = (y_proba >= best_thresh).astype(int)\n",
    "    metrics = compute_metrics(y_val, y_pred, y_proba)\n",
    "    \n",
    "    return best_thresh, metrics\n",
    "\n",
    "def get_confusion_matrix_str(y_true, y_pred):\n",
    "    \"\"\"Return confusion matrix as formatted string.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    return f'TP={tp}, FP={fp}, TN={tn}, FN={fn}'\n",
    "\n",
    "print('Helper functions defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============ RESULT ROW BUILDER ============\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def build_result_row(\n",
    "    exp_id,\n",
    "    config,\n",
    "    train_default,\n",
    "    val_default,\n",
    "    test_default,\n",
    "    best_thresh,\n",
    "    val_tuned,\n",
    "    test_tuned,\n",
    "    val_cm,\n",
    "    test_cm,\n",
    "    train_time,\n",
    "    early_stop_epoch,\n",
    "    converged,\n",
    "):\n",
    "    \"\"\"Build a single result row for the experiments CSV.\"\"\"\n",
    "    tn_val, fp_val, fn_val, tp_val = val_cm.ravel()\n",
    "    tn_test, fp_test, fn_test, tp_test = test_cm.ravel()\n",
    "\n",
    "    constraint_met = (val_tuned[\"recall\"] >= RECALL_CONSTRAINT) and (\n",
    "        test_tuned[\"recall\"] >= RECALL_CONSTRAINT\n",
    "    )\n",
    "    is_leader = constraint_met and (test_tuned[\"accuracy\"] > BASELINE_TEST_ACC)\n",
    "\n",
    "    return {\n",
    "        \"id\": exp_id,\n",
    "        \"hidden_layer_sizes\": str(config[\"hidden_layer_sizes\"]),\n",
    "        \"alpha\": config[\"alpha\"],\n",
    "        \"learning_rate_init\": config[\"learning_rate_init\"],\n",
    "        \"max_iter\": config[\"max_iter\"],\n",
    "        \"best_threshold\": best_thresh,\n",
    "        # Default threshold = 0.5 metrics\n",
    "        \"train_acc_05\": train_default[\"accuracy\"],\n",
    "        \"train_rec_05\": train_default[\"recall\"],\n",
    "        \"train_prec_05\": train_default[\"precision\"],\n",
    "        \"train_auc_05\": train_default[\"auc\"],\n",
    "        \"val_acc_05\": val_default[\"accuracy\"],\n",
    "        \"val_rec_05\": val_default[\"recall\"],\n",
    "        \"val_prec_05\": val_default[\"precision\"],\n",
    "        \"val_auc_05\": val_default[\"auc\"],\n",
    "        \"test_acc_05\": test_default[\"accuracy\"],\n",
    "        \"test_rec_05\": test_default[\"recall\"],\n",
    "        \"test_prec_05\": test_default[\"precision\"],\n",
    "        \"test_auc_05\": test_default[\"auc\"],\n",
    "        # Tuned threshold metrics\n",
    "        \"val_acc_tuned\": val_tuned[\"accuracy\"],\n",
    "        \"val_rec_tuned\": val_tuned[\"recall\"],\n",
    "        \"val_prec_tuned\": val_tuned[\"precision\"],\n",
    "        \"val_auc_tuned\": val_tuned[\"auc\"],\n",
    "        \"test_acc_tuned\": test_tuned[\"accuracy\"],\n",
    "        \"test_rec_tuned\": test_tuned[\"recall\"],\n",
    "        \"test_prec_tuned\": test_tuned[\"precision\"],\n",
    "        \"test_auc_tuned\": test_tuned[\"auc\"],\n",
    "        # Confusion matrices at tuned threshold\n",
    "        \"val_tn\": tn_val,\n",
    "        \"val_fp\": fp_val,\n",
    "        \"val_fn\": fn_val,\n",
    "        \"val_tp\": tp_val,\n",
    "        \"test_tn\": tn_test,\n",
    "        \"test_fp\": fp_test,\n",
    "        \"test_fn\": fn_test,\n",
    "        \"test_tp\": tp_test,\n",
    "        # Meta\n",
    "        \"train_time\": train_time,\n",
    "        \"early_stop_epoch\": early_stop_epoch,\n",
    "        \"converged\": bool(converged),\n",
    "        \"constraint_met\": bool(constraint_met),\n",
    "        \"is_leader\": bool(is_leader),\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Result row builder defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search: 80 experiments\n",
      "============================================================\n",
      "[1/80] (128, 64, 32), α=1e-05, lr=0.001, max_iter=300 ... "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_experiment_markdown' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m test_tuned = compute_metrics(y_test, y_pred_test, y_proba_test)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Generate markdown and collect summary\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m md_content, summary = \u001b[43mgenerate_experiment_markdown\u001b[49m(\n\u001b[32m     64\u001b[39m     exp_id, config, train_metrics, val_metrics, test_metrics,\n\u001b[32m     65\u001b[39m     best_thresh, val_tuned, test_tuned, train_time, early_stop_epoch, converged\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Save markdown file\u001b[39;00m\n\u001b[32m     69\u001b[39m md_path = OBSIDIAN_DIR / \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.md\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'generate_experiment_markdown' is not defined"
     ]
    }
   ],
   "source": [
    "# ============ RUN FULL GRID SEARCH ============\n",
    "from itertools import product\n",
    "\n",
    "all_results = []\n",
    "experiment_count = 0\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"Starting grid search: {total_experiments} experiments\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for hidden, alpha, lr, max_iter in product(\n",
    "    HIDDEN_LAYER_SIZES, ALPHAS, LEARNING_RATES, MAX_ITERS\n",
    "):\n",
    "    experiment_count += 1\n",
    "\n",
    "    # Generate experiment ID\n",
    "    exp_id = f\"mlp_exp_{start_time.strftime('%Y%m%d_%H%M')}_model{experiment_count:03d}\"\n",
    "    config = {\n",
    "        \"hidden_layer_sizes\": hidden,\n",
    "        \"alpha\": alpha,\n",
    "        \"learning_rate_init\": lr,\n",
    "        \"max_iter\": max_iter,\n",
    "    }\n",
    "\n",
    "    # Progress\n",
    "    print(\n",
    "        f\"[{experiment_count}/{total_experiments}] {hidden}, α={alpha}, lr={lr}, max_iter={max_iter}\",\n",
    "        end=\" ... \",\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    t0 = time.time()\n",
    "    mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=hidden,\n",
    "        alpha=alpha,\n",
    "        learning_rate_init=lr,\n",
    "        max_iter=max_iter,\n",
    "        activation=\"relu\",\n",
    "        solver=\"adam\",\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1,\n",
    "        batch_size=\"auto\",\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    with warnings.catch_warnings(record=True) as w:\n",
    "        warnings.simplefilter(\"always\")\n",
    "        mlp.fit(X_train_s, y_train)\n",
    "        converged = not any(\"ConvergenceWarning\" in str(warning.category) for warning in w)\n",
    "\n",
    "    train_time = time.time() - t0\n",
    "    early_stop_epoch = mlp.n_iter_ if hasattr(mlp, \"n_iter_\") else \"N/A\"\n",
    "\n",
    "    # Evaluate at default threshold 0.5\n",
    "    train_default = eval_model_at_threshold(mlp, X_train_s, y_train, 0.5)\n",
    "    val_default = eval_model_at_threshold(mlp, X_val_s, y_val, 0.5)\n",
    "    test_default = eval_model_at_threshold(mlp, X_test_s, y_test, 0.5)\n",
    "\n",
    "    # Threshold tuning\n",
    "    best_thresh, val_tuned = find_best_threshold(\n",
    "        mlp, X_val_s, y_val, RECALL_CONSTRAINT\n",
    "    )\n",
    "\n",
    "    # Validation metrics & confusion at tuned threshold\n",
    "    y_proba_val = mlp.predict_proba(X_val_s)[:, 1]\n",
    "    y_pred_val_tuned = (y_proba_val >= best_thresh).astype(int)\n",
    "    val_cm = confusion_matrix(y_val, y_pred_val_tuned)\n",
    "\n",
    "    # Test metrics & confusion at tuned threshold\n",
    "    y_proba_test = mlp.predict_proba(X_test_s)[:, 1]\n",
    "    y_pred_test_tuned = (y_proba_test >= best_thresh).astype(int)\n",
    "    test_tuned = compute_metrics(y_test, y_pred_test_tuned, y_proba_test)\n",
    "    test_cm = confusion_matrix(y_test, y_pred_test_tuned)\n",
    "\n",
    "    # Build result row and collect\n",
    "    row = build_result_row(\n",
    "        exp_id,\n",
    "        config,\n",
    "        train_default,\n",
    "        val_default,\n",
    "        test_default,\n",
    "        best_thresh,\n",
    "        val_tuned,\n",
    "        test_tuned,\n",
    "        val_cm,\n",
    "        test_cm,\n",
    "        train_time,\n",
    "        early_stop_epoch,\n",
    "        converged,\n",
    "    )\n",
    "    all_results.append(row)\n",
    "\n",
    "    status = \"✅\" if row[\"is_leader\"] else (\"⚠️\" if row[\"constraint_met\"] else \"❌\")\n",
    "    print(\n",
    "        f\"{status} Acc={row['test_acc_tuned']:.4f} Rec={row['test_rec_tuned']:.4f} ({train_time:.1f}s)\"\n",
    "    )\n",
    "\n",
    "# Build DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds()\n",
    "print(\"=\" * 60)\n",
    "print(f\"Grid search complete in {total_time:.1f}s\")\n",
    "print(f\"Total experiments: {len(results_df)}\")\n",
    "print(\n",
    "    f\"Valid candidates (Recall >= {RECALL_CONSTRAINT}): {int(results_df['constraint_met'].sum())}\"\n",
    ")\n",
    "print(\n",
    "    f\"Leader candidates (beat baseline): {int(results_df['is_leader'].sum())}\"\n",
    ")\n",
    "\n",
    "# Save full results as CSV\n",
    "csv_path = RESULTS_DIR / \"all_experiments.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"Full results saved to: {csv_path}\")\n",
    "\n",
    "# Show top 10 valid candidates (by tuned metrics)\n",
    "valid_df = results_df[results_df[\"constraint_met\"]].copy()\n",
    "if not valid_df.empty:\n",
    "    valid_df = valid_df.sort_values(\n",
    "        by=[\"val_acc_tuned\", \"test_acc_tuned\", \"test_auc_tuned\"],\n",
    "        ascending=[False, False, False],\n",
    "    )\n",
    "\n",
    "    top_10 = valid_df.head(10)[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"hidden_layer_sizes\",\n",
    "            \"alpha\",\n",
    "            \"learning_rate_init\",\n",
    "            \"best_threshold\",\n",
    "            \"val_acc_tuned\",\n",
    "            \"val_rec_tuned\",\n",
    "            \"test_acc_tuned\",\n",
    "            \"test_rec_tuned\",\n",
    "            \"test_auc_tuned\",\n",
    "            \"is_leader\",\n",
    "        ]\n",
    "    ]\n",
    "    print(\"\\nTop 10 valid candidates (tuned threshold):\")\n",
    "    print(top_10.round(4).to_string(index=False))\n",
    "else:\n",
    "    print(\"\\nNo valid candidates found that meet the recall constraint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsidian leaderboard generation has been moved to a separate script:\n",
    "# `scripts/generate_mlp_obsidian_notes.py`\n",
    "# This cell is intentionally left empty.\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training candidate: baseline_128-64-32_a1e-4\n",
      "\n",
      "Training candidate: 256-128-64_a1e-4\n",
      "\n",
      "Training candidate: 256-128-64_a1e-3\n",
      "\n",
      "Training candidate: 128-64_a1e-3\n",
      "\n",
      "MLP small grid – Train/Val/Test metrics\n",
      "                    Name hidden_layer_sizes  alpha  Train_acc  Train_rec  Val_acc  Val_rec  Test_acc  Test_rec\n",
      "baseline_128-64-32_a1e-4      (128, 64, 32) 0.0001     0.9436     0.9154   0.9049   0.8345    0.9082    0.8466\n",
      "        256-128-64_a1e-4     (256, 128, 64) 0.0001     0.9472     0.9143   0.9078   0.8259    0.9124    0.8552\n",
      "        256-128-64_a1e-3     (256, 128, 64) 0.0010     0.9507     0.9361   0.9119   0.8517    0.9136    0.8793\n",
      "            128-64_a1e-3          (128, 64) 0.0010     0.9489     0.9191   0.9053   0.8172    0.9111    0.8414\n",
      "\n",
      "No candidate reached Val & Test recall >= 0.919 (tuned baseline).\n"
     ]
    }
   ],
   "source": [
    "# Obsidian markdown generation is now handled by `scripts/generate_mlp_obsidian_notes.py`.\n",
    "# Nothing to do in this cell.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final analysis and recommendation can be performed using the\n",
    "# leaderboard generated by the Obsidian script.\n",
    "# See: obsidian_notes/experiments_mlp/summaries/mlp_leaderboard.md\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen threshold = 0.31 with Val Acc = 0.9032 and Val Recall >= 0.919\n",
      "Test Accuracy: 0.9016 Test Recall: 0.9259\n"
     ]
    }
   ],
   "source": [
    "# Threshold tuning notes and CSV export are now produced directly\n",
    "# from the results CSV by `scripts/generate_mlp_obsidian_notes.py`.\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576f1c5",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Open Obsidian** and point it at `CardioDetect/obsidian_notes/`\n",
    "2. **Review the leaderboard** at `experiments_mlp/summaries/mlp_leaderboard.md`\n",
    "3. **If a candidate beats baseline**:\n",
    "   - Open its experiment note for full details\n",
    "   - Update `00_complete_project_walkthrough.ipynb` with the new best model\n",
    "   - Save the model to `models/` with a new name\n",
    "4. **If no candidate beats baseline**:\n",
    "   - Keep `mlp_v2` as the production model\n",
    "   - Document findings in `MY_NOTES.md`\n",
    "\n",
    "---\n",
    "\n",
    "**Grid Search Summary**:\n",
    "- 80 configurations tested (5 architectures × 4 alphas × 2 LRs × 2 max_iters)\n",
    "- Threshold tuning applied to each with recall constraint ≥ 0.9190\n",
    "- Results ranked by validation accuracy, then test accuracy, then AUC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
